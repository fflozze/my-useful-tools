# LM Studio

> [!TIP]
> [Passer à la version française](#version-française)

## Description
LM Studio is a desktop application that allows you to discover, download, and run local LLMs (Large Language Models) on your own machine. It provides a simple interface to interact with models like Llama, Mistral, and many others.

## Website / Link
- [Official Website](https://lmstudio.ai/)
- [Hugging Face Models](https://huggingface.co/models?other=llm)

## Installation
```bash
# Download the installer for Windows, macOS, or Linux from the official website.
# Install like a standard application.
```

## Useful Commands / Tips
- Search for "GGUF" models on Hugging Face to find compatible formats.
- Use the "Local Server" feature to expose an OpenAI-compatible API to other applications.

## Configuration
- In "Settings", you can configure GPU acceleration (Metal on Mac, CUDA or Vulkan on Windows/Linux).
- You can change the "Context Length" depending on your RAM/VRAM.

## Notes
Perfect for testing models privately without sending data to external servers.

---

# Version Française

## Description
LM Studio est une application de bureau qui vous permet de découvrir, télécharger et exécuter des LLM (Large Language Models) locaux sur votre propre machine. Elle offre une interface simple pour interagir avec des modèles comme Llama, Mistral et bien d'autres.

## Site Web / Lien
- [Site Officiel](https://lmstudio.ai/)
- [Modèles Hugging Face](https://huggingface.co/models?other=llm)

## Installation
```bash
# Téléchargez l'installateur pour Windows, macOS ou Linux depuis le site officiel.
# Installez comme une application standard.
```

## Commandes Utiles / Astuces
- Recherchez des modèles "GGUF" sur Hugging Face pour trouver des formats compatibles.
- Utilisez la fonction "Local Server" pour exposer une API compatible OpenAI à d'autres applications.

## Configuration
- Dans "Settings", vous pouvez configurer l'accélération GPU (Metal sur Mac, CUDA ou Vulkan sur Windows/Linux).
- Vous pouvez modifier la "Context Length" (longueur du contexte) en fonction de votre RAM/VRAM.

## Notes
Parfait pour tester des modèles de manière privée sans envoyer de données à des serveurs externes.
